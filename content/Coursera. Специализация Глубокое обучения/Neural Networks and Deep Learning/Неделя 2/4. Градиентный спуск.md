---
title: 4. Градиентный спуск
---
Вспоминаем: функция логистической регрессии и функции стоимости 

Хотим найти  w и b, которые минимизируют J(w, b)(стоимость).  
Функция J - выпуклая(как чаша) => можно найти минимум. Как это сделать?  
С помощью градиентного спуска!   

Сначала нужно выбрать рандомную точку, потом с помощью градиентного спускаться всё ниже и ниже. 

$$
\displaylines{
Repeat( \\ \\
w:= w - \alpha \frac{d J(w)}{dw}
\\
b:= b - \alpha \frac{d J(w)}{dw}
\\ \\
)
\\
\alpha - learning \space rate
\\
w := w - \alpha dw
\\
b:= b - \alpha db
\\
\frac{d J(w)}{dw} - наклон \space функции
}
$$


